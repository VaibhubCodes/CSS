This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-06-27T04:40:29.099Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
management/
  commands/
    fix_missing_storage.py
    refresh_storage_usage.py
migrations/
  0001_initial.py
  0002_alter_userstorage_options_adminaccesslog.py
admin.py
apps.py
models.py
serializers.py
tests.py
urls.py
utils.py
views.py

================================================================
Repository Files
================================================================

================
File: management/commands/fix_missing_storage.py
================
from django.core.management.base import BaseCommand
from django.contrib.auth import get_user_model
from storage_management.models import UserStorage

class Command(BaseCommand):
    help = 'Fix missing UserStorage records for existing users'

    def add_arguments(self, parser):
        parser.add_argument(
            '--dry-run',
            action='store_true',
            help='Show what would be done without making changes',
        )

    def handle(self, *args, **options):
        User = get_user_model()
        dry_run = options['dry_run']
        
        self.stdout.write(self.style.SUCCESS('Checking for users without UserStorage records...'))
        
        users_without_storage = []
        for user in User.objects.all():
            try:
                UserStorage.objects.get(user=user)
                self.stdout.write(f'âœ… {user.email} has storage record')
            except UserStorage.DoesNotExist:
                self.stdout.write(self.style.WARNING(f'âŒ {user.email} MISSING storage record'))
                users_without_storage.append(user)

        if not users_without_storage:
            self.stdout.write(self.style.SUCCESS('\nâœ… All users have storage records'))
            return

        self.stdout.write(f'\nFound {len(users_without_storage)} users without storage records')
        
        if dry_run:
            self.stdout.write(self.style.WARNING('DRY RUN MODE - No changes will be made'))
            for user in users_without_storage:
                self.stdout.write(f'Would create storage record for {user.email}')
        else:
            created_count = 0
            for user in users_without_storage:
                storage, created = UserStorage.objects.get_or_create(
                    user=user,
                    defaults={
                        'storage_used': 0,
                        'storage_limit': 5368709120  # 5GB default
                    }
                )
                if created:
                    # Update storage limit based on subscription if available
                    try:
                        storage.update_from_subscription()
                    except Exception as e:
                        self.stdout.write(
                            self.style.WARNING(f'Could not update subscription info for {user.email}: {e}')
                        )
                    
                    self.stdout.write(f'âœ… Created storage record for {user.email}')
                    created_count += 1
                else:
                    self.stdout.write(f'âš ï¸  Storage record already exists for {user.email}')

            self.stdout.write(
                self.style.SUCCESS(f'\nâœ… Successfully created {created_count} storage records')
            )

================
File: management/commands/refresh_storage_usage.py
================
from django.core.management.base import BaseCommand
from django.contrib.auth import get_user_model
from storage_management.models import UserStorage
from storage_management.utils import S3StorageManager

class Command(BaseCommand):
    help = 'Refresh storage usage for all users from S3'

    def add_arguments(self, parser):
        parser.add_argument(
            '--user',
            type=str,
            help='Refresh storage for specific user (email)',
        )
        parser.add_argument(
            '--dry-run',
            action='store_true',
            help='Show what would be done without making changes',
        )

    def handle(self, *args, **options):
        User = get_user_model()
        dry_run = options['dry_run']
        specific_user = options['user']
        
        if specific_user:
            try:
                users = [User.objects.get(email=specific_user)]
                self.stdout.write(f'Refreshing storage for user: {specific_user}')
            except User.DoesNotExist:
                self.stdout.write(self.style.ERROR(f'User {specific_user} not found'))
                return
        else:
            users = User.objects.all()
            self.stdout.write(f'Refreshing storage for all {users.count()} users...')
        
        if dry_run:
            self.stdout.write(self.style.WARNING('DRY RUN MODE - No changes will be made'))
        
        updated_count = 0
        error_count = 0
        
        for user in users:
            try:
                # Ensure user has a storage record
                storage, created = UserStorage.objects.get_or_create(
                    user=user,
                    defaults={
                        'storage_used': 0,
                        'storage_limit': 5368709120  # 5GB default
                    }
                )
                
                if created:
                    self.stdout.write(f'Created missing storage record for {user.email}')
                
                # Get current usage from database
                old_usage = storage.storage_used
                
                # Calculate actual usage from S3
                storage_manager = S3StorageManager(user)
                
                if dry_run:
                    # Just show what would happen
                    try:
                        storage_info = storage_manager.get_user_storage_info()
                        new_usage = storage_info['used']
                        self.stdout.write(
                            f'ðŸ“Š {user.email}: {old_usage} bytes â†’ {new_usage} bytes '
                            f'(Î” {new_usage - old_usage:+d} bytes)'
                        )
                    except Exception as e:
                        self.stdout.write(self.style.ERROR(f'âŒ {user.email}: Error calculating - {e}'))
                        error_count += 1
                else:
                    # Actually update the storage
                    storage_info = storage_manager.get_user_storage_info()
                    new_usage = storage_info['used']
                    
                    self.stdout.write(
                        f'âœ… {user.email}: {old_usage} bytes â†’ {new_usage} bytes '
                        f'(Î” {new_usage - old_usage:+d} bytes)'
                    )
                    updated_count += 1
                
            except Exception as e:
                self.stdout.write(self.style.ERROR(f'âŒ {user.email}: {e}'))
                error_count += 1
        
        if not dry_run:
            self.stdout.write(
                self.style.SUCCESS(f'\nâœ… Successfully updated {updated_count} users')
            )
        
        if error_count > 0:
            self.stdout.write(
                self.style.WARNING(f'âš ï¸  {error_count} errors encountered')
            )

================
File: migrations/0001_initial.py
================
# Generated by Django 5.1.1 on 2024-11-27 04:21

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='UserStorage',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('storage_used', models.BigIntegerField(default=0)),
                ('storage_limit', models.BigIntegerField(default=5368709120)),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
                ('user', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),
            ],
        ),
    ]

================
File: migrations/0002_alter_userstorage_options_adminaccesslog.py
================
# Generated by Django 5.1.3 on 2024-12-27 09:56

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('storage_management', '0001_initial'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.AlterModelOptions(
            name='userstorage',
            options={'verbose_name_plural': 'User Storage'},
        ),
        migrations.CreateModel(
            name='AdminAccessLog',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('accessed_file', models.CharField(max_length=255)),
                ('access_time', models.DateTimeField(auto_now_add=True)),
                ('ip_address', models.GenericIPAddressField(blank=True, null=True)),
                ('access_type', models.CharField(max_length=50)),
                ('admin_user', models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL)),
            ],
            options={
                'ordering': ['-access_time'],
            },
        ),
    ]

================
File: admin.py
================
from django.contrib import admin
from django.utils.html import format_html
from .models import UserStorage, AdminAccessLog

@admin.register(UserStorage)
class UserStorageAdmin(admin.ModelAdmin):
    """
    Admin view for UserStorage.
    Provides a read-only interface with formatted data and a visual usage bar.
    """
    list_display = ('user', 'formatted_storage_used', 'formatted_storage_limit', 'usage_bar', 'updated_at')
    search_fields = ('user__email', 'user__username')
    readonly_fields = ('user', 'storage_used', 'storage_limit', 'created_at', 'updated_at', 'usage_bar')
    list_per_page = 25

    def formatted_storage_used(self, obj):
        return self.format_size(obj.storage_used)
    formatted_storage_used.short_description = 'Storage Used'

    def formatted_storage_limit(self, obj):
        return self.format_size(obj.storage_limit)
    formatted_storage_limit.short_description = 'Storage Limit'

    def usage_bar(self, obj):
        """Creates a visual progress bar for storage usage."""
        if obj.storage_limit == 0:
            return "N/A"
        
        percentage = obj.get_usage_percentage()
        color = 'green'
        if percentage > 90:
            color = 'red'
        elif percentage > 75:
            color = 'orange'
        
        # --- FIX APPLIED HERE ---
        # Pre-format the percentage into a string first.
        percentage_text = f"{percentage:.1f}%"
        
        # Then, use a simple placeholder {} for the pre-formatted text.
        return format_html(
            '<div style="width: 100%; border: 1px solid #ccc; background: #f0f0f0; border-radius: 4px;">'
            '<div style="height: 18px; width: {}%; background-color: {}; border-radius: 4px; text-align: center; color: white; font-weight: bold;">{}</div>'
            '</div>',
            percentage,  # This is for the width calculation (needs to be a number)
            color,
            percentage_text  # This is for the display text (now a simple string)
        )
    usage_bar.short_description = 'Usage %'

    @staticmethod
    def format_size(size_in_bytes):
        if size_in_bytes is None:
            return "0 B"
        if size_in_bytes == 0:
            return "0 B"
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_in_bytes < 1024.0:
                return f"{size_in_bytes:.2f} {unit}"
            size_in_bytes /= 1024.0
        return f"{size_in_bytes:.2f} PB"

    def has_add_permission(self, request):
        # UserStorage is created automatically via signal
        return False

    def has_delete_permission(self, request, obj=None):
        return False

@admin.register(AdminAccessLog)
class AdminAccessLogAdmin(admin.ModelAdmin):
    """
    Read-only admin view for auditing admin access to user files.
    """
    list_display = ('admin_user', 'accessed_file', 'access_time', 'ip_address', 'access_type')
    list_filter = ('access_time', 'admin_user', 'access_type')
    search_fields = ('admin_user__username', 'accessed_file', 'ip_address')
    readonly_fields = ('admin_user', 'accessed_file', 'access_time', 'ip_address', 'access_type')
    date_hierarchy = 'access_time'

    def has_add_permission(self, request):
        return False

    def has_change_permission(self, request, obj=None):
        return False

    def has_delete_permission(self, request, obj=None):
        return False

================
File: apps.py
================
from django.apps import AppConfig

class StorageManagementConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'storage_management'

    def ready(self):
        import storage_management.models  # This imports the signals

================
File: models.py
================
from django.db import models
from django.conf import settings
import boto3
from django.db.models.signals import post_save
from django.dispatch import receiver
from django.contrib.auth import get_user_model

class UserStorage(models.Model):
    user = models.OneToOneField(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)
    storage_used = models.BigIntegerField(default=0)  # in bytes
    storage_limit = models.BigIntegerField(default=5368709120)  # 5GB in bytes (default)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    def get_usage_percentage(self):
        if self.storage_limit == 0:
            return 0
        return (self.storage_used / self.storage_limit) * 100

    def get_available_storage(self):
        return max(0, self.storage_limit - self.storage_used)

    def __str__(self):
        return f"{self.user.email}'s Storage"
    
    def update_from_subscription(self):
        """Update storage limit based on user's active subscription"""
        try:
            from payments.utils import get_user_subscription_info
            subscription_info = get_user_subscription_info(self.user)
            
            # Convert GB to bytes
            new_limit = subscription_info['storage_limit_gb'] * 1024 * 1024 * 1024
            
            if self.storage_limit != new_limit:
                self.storage_limit = new_limit
                self.save()
                return True
        except Exception as e:
            print(f"Error updating storage from subscription: {str(e)}")
        return False
    
    class Meta:
        verbose_name_plural = "User Storage"

@receiver(post_save, sender=get_user_model())
def create_user_storage(sender, instance, created, **kwargs):
    """Create UserStorage instance for new users"""
    if created:
        try:
            user_storage, storage_created = UserStorage.objects.get_or_create(
                user=instance,
                defaults={
                    'storage_used': 0,
                    'storage_limit': 5368709120  # 5GB default
                }
            )
            if storage_created:
                print(f"[SIGNAL] Created UserStorage for user {instance.email} (ID: {instance.id})")
                # Update storage limit based on any existing subscription
                try:
                    user_storage.update_from_subscription()
                except Exception as e:
                    print(f"[SIGNAL] Could not update subscription for {instance.email}: {e}")
            else:
                print(f"[SIGNAL] UserStorage already exists for user {instance.email} (ID: {instance.id})")
        except Exception as e:
            print(f"[SIGNAL ERROR] Failed to create UserStorage for {instance.email}: {e}")

# Signal to update storage when subscription changes
@receiver(post_save, sender='payments.Subscription')
def update_storage_on_subscription_change(sender, instance, created, **kwargs):
    """Update user storage limit when subscription status changes"""
    if instance.status == 'active':
        try:
            user_storage, storage_created = UserStorage.objects.get_or_create(user=instance.user)
            user_storage.storage_limit = instance.plan.storage_bytes
            user_storage.save()
        except Exception as e:
            print(f"Error updating storage on subscription change: {str(e)}")

class AdminAccessLog(models.Model):
    admin_user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, null=True)
    accessed_file = models.CharField(max_length=255)
    access_time = models.DateTimeField(auto_now_add=True)
    ip_address = models.GenericIPAddressField(null=True, blank=True)
    access_type = models.CharField(max_length=50)  # e.g., 'view', 'download', 'delete'

    class Meta:
        ordering = ['-access_time']

    def __str__(self):
        return f"{self.admin_user} accessed {self.accessed_file} at {self.access_time}"

================
File: serializers.py
================
from rest_framework import serializers
from .models import UserStorage, AdminAccessLog

class StorageInfoSerializer(serializers.ModelSerializer):
    usage_percentage = serializers.SerializerMethodField()
    storage_used_formatted = serializers.SerializerMethodField()
    storage_limit_formatted = serializers.SerializerMethodField()
    available_storage_formatted = serializers.SerializerMethodField()

    class Meta:
        model = UserStorage
        fields = [
            'id', 'storage_used', 'storage_limit', 'usage_percentage',
            'storage_used_formatted', 'storage_limit_formatted',
            'available_storage_formatted', 'created_at', 'updated_at'
        ]
        read_only_fields = ['storage_used', 'created_at', 'updated_at']

    def get_usage_percentage(self, obj):
        return obj.get_usage_percentage()

    def format_size(self, size_in_bytes):
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_in_bytes < 1024:
                return f"{size_in_bytes:.2f} {unit}"
            size_in_bytes /= 1024
        return f"{size_in_bytes:.2f} TB"

    def get_storage_used_formatted(self, obj):
        return self.format_size(obj.storage_used)

    def get_storage_limit_formatted(self, obj):
        return self.format_size(obj.storage_limit)

    def get_available_storage_formatted(self, obj):
        return self.format_size(obj.get_available_storage())

class StorageAnalyticsSerializer(serializers.Serializer):
    file_count = serializers.IntegerField()
    file_types = serializers.DictField()
    categories = serializers.DictField()
    recent_uploads = serializers.ListField()
    storage_growth = serializers.DictField()

class AdminAccessLogSerializer(serializers.ModelSerializer):
    admin_user = serializers.StringRelatedField()

    class Meta:
        model = AdminAccessLog
        fields = [
            'id', 'admin_user', 'accessed_file', 'access_time',
            'ip_address', 'access_type'
        ]
        read_only_fields = fields

class StorageOptimizationSerializer(serializers.Serializer):
    large_files = serializers.ListField()
    duplicate_files = serializers.ListField()
    old_files = serializers.ListField()
    potential_savings = serializers.IntegerField()
    recommendations = serializers.ListField()

class EnhancedStorageInfoSerializer(StorageInfoSerializer):
    """Enhanced serializer with subscription info"""
    subscription_info = serializers.SerializerMethodField()
    
    class Meta(StorageInfoSerializer.Meta):
        fields = StorageInfoSerializer.Meta.fields + ['subscription_info']
    
    def get_subscription_info(self, obj):
        try:
            from payments.utils import get_user_subscription_info
            return get_user_subscription_info(obj.user)
        except ImportError:
            return {'is_sparkle': False}

================
File: tests.py
================
from django.test import TestCase

# Create your tests here.

================
File: urls.py
================
from django.urls import path, include
from rest_framework.routers import DefaultRouter
from . import views

router = DefaultRouter()
router.register(r'storage', views.StorageViewSet, basename='storage')
router.register(r'admin-logs', views.AdminAccessLogViewSet, basename='admin-logs')

urlpatterns = [
    path('api/', include(router.urls)),
    path('info/', views.get_storage_info, name='storage_info'),

]

================
File: utils.py
================
import boto3,datetime
from django.conf import settings
from .models import UserStorage
from django.utils import timezone
from storage_management.models import AdminAccessLog 
import logging
logger = logging.getLogger(__name__)
def log_admin_access(user, file_key):
    """Log when admin accesses files through AWS console"""
    if settings.AWS_LOGGING:
        admin_log = AdminAccessLog.objects.create(
            admin_user=user,
            accessed_file=file_key,
            access_time=timezone.now()
        )
        return admin_log

class S3StorageManager:
    def __init__(self, user):
        self.user = user
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
            aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
            region_name=settings.AWS_S3_REGION_NAME
        )
        self.bucket_name = settings.AWS_STORAGE_BUCKET_NAME
        self.user_prefix = f"user_{user.id}/"

    def get_user_storage_info(self):
        """Get current storage usage for user"""
        try:
            paginator = self.s3_client.get_paginator('list_objects_v2')
            total_size = 0
            
            # Iterate through all objects in user's prefix
            for page in paginator.paginate(
                Bucket=self.bucket_name,
                Prefix=self.user_prefix
            ):
                if 'Contents' in page:
                    total_size += sum(obj['Size'] for obj in page['Contents'])
            
            # Update storage usage in database
            storage, created = UserStorage.objects.get_or_create(
                user=self.user,
                defaults={
                    'storage_used': 0,
                    'storage_limit': 5368709120  # 5GB default
                }
            )
            if created:
                print(f"[S3StorageManager] Created missing UserStorage for user {self.user.email} (ID: {self.user.id})")
                # Update storage limit based on subscription if available
                try:
                    storage.update_from_subscription()
                except Exception as e:
                    print(f"[S3StorageManager] Could not update subscription for {self.user.email}: {e}")
            
            storage.storage_used = total_size
            storage.save()
            
            return {
                'used': total_size,
                'limit': storage.storage_limit,
                'available': storage.get_available_storage(),
                'percentage_used': storage.get_usage_percentage()
            }
        except Exception as e:
            raise Exception(f"Error getting storage info: {str(e)}")

    def get_file_url(self, s3_key, expiry=3600, response_content_disposition=None):
        """
        Generate a presigned URL for accessing an S3 object.

        Args:
            s3_key (str): The full S3 key (path) of the object within the bucket
                          (e.g., 'user_1/documents/report.pdf').
            expiry (int): Duration in seconds for which the URL should be valid.
                          Defaults to 3600 seconds (1 hour).
            response_content_disposition (str, optional): Sets the Content-Disposition header
                          for the response. Useful for forcing download with a specific filename
                          (e.g., 'attachment; filename="your_filename.pdf"'). Defaults to None.


        Returns:
            str or None: The presigned URL if successful, None otherwise.
        """
        # Ensure the S3 client was initialized correctly
        if not self.s3_client or not self.bucket_name:
            logger.error(f"S3 client or bucket name not initialized for user {self.user.id}. Cannot generate URL for key '{s3_key}'.")
            return None

        # Validate the s3_key format (optional but good practice)
        if not s3_key or not isinstance(s3_key, str):
             logger.error(f"Invalid S3 key provided for URL generation: {s3_key}")
             return None
        # You might add more specific checks, e.g., ensuring it contains the user prefix if required by policy
        # if not s3_key.startswith(self.user_prefix):
        #     logger.warning(f"S3 key '{s3_key}' might not be within the expected user prefix '{self.user_prefix}'.")

        try:
            logger.debug(f"Generating presigned URL for user {self.user.id}, key: '{s3_key}', expiry: {expiry} seconds.")

            # Prepare parameters for generate_presigned_url
            params = {
                'Bucket': self.bucket_name,
                'Key': s3_key
            }

            # Add Content-Disposition if requested (for download links)
            if response_content_disposition:
                params['ResponseContentDisposition'] = response_content_disposition
                logger.debug(f"Setting ResponseContentDisposition: {response_content_disposition}")


            # Generate the presigned URL for a GET request
            url = self.s3_client.generate_presigned_url(
                ClientMethod='get_object',
                Params=params,
                ExpiresIn=expiry # Pass the expiry duration here
            )

            logger.info(f"Successfully generated presigned URL for key '{s3_key}' (expires in {expiry}s).")
            logger.debug(f"Generated URL (first 100 chars): {url[:100]}...") # Log part of URL for verification
            return url

        except Exception as e:
            # Catch potential errors from boto3 (e.g., credentials error, bucket not found, key not found (less likely for get_object URL generation))
            logger.exception(f"Error generating presigned URL for user {self.user.id}, key '{s3_key}': {str(e)}")
            return None

    def get_user_files(self):
        """List all files in user's S3 directory"""
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=self.user_prefix
            )
            return response.get('Contents', [])
        except Exception as e:
            raise Exception(f"Error listing user files: {str(e)}")

    def check_storage_limit(self, file_size):
        """Check if uploading file would exceed storage limit"""
        storage = UserStorage.objects.get(user=self.user)
        return (storage.storage_used + file_size) <= storage.storage_limit

    # def upload_file(self, file_obj, file_name):
    #     """Upload file to user's S3 directory"""
    #     if hasattr(file_obj, 'size'):
    #         file_size = file_obj.size
    #     else:
    #         file_obj.seek(0, 2)  # Seek to end
    #         file_size = file_obj.tell()
    #         file_obj.seek(0)  # Seek back to start
            
    #     if not self.check_storage_limit(file_size):
    #         raise Exception("Storage limit would be exceeded")

    #     try:
    #         # Generate S3 key with user prefix
    #         s3_key = f"{self.user_prefix}{file_name}"
            
    #         # Upload file
    #         self.s3_client.upload_fileobj(
    #             file_obj,
    #             self.bucket_name,
    #             s3_key
    #         )
            
    #         # Update storage usage
    #         storage = UserStorage.objects.get(user=self.user)
    #         storage.storage_used += file_size
    #         storage.save()
            
    #         return s3_key
    #     except Exception as e:
    #         raise Exception(f"Error uploading file: {str(e)}")

    def upload_file(self, file_obj, file_name):
            """Upload file to user's S3 directory"""
            try:
                s3_key = f"{self.user_prefix}{file_name}"
                
                # Upload with private ACL
                self.s3_client.upload_fileobj(
                    file_obj,
                    self.bucket_name,
                    s3_key,
                    ExtraArgs={
                        'ACL': 'private',
                        'Metadata': {
                            'user_id': str(self.user.id),
                            'upload_date': timezone.now().isoformat()  
                        }
                    }
                )
                
                # Update storage usage
                storage = UserStorage.objects.get(user=self.user)
                storage.storage_used += getattr(file_obj, 'size', 0)
                storage.save()
                
                return s3_key
            except Exception as e:
                print(f"Upload error: {str(e)}")  # Add logging
                raise Exception(f"Error uploading file: {str(e)}")


    # def delete_file(self, file_name):
    #     """Delete file from user's S3 directory"""
    #     try:
    #         # Get file size before deletion
    #         response = self.s3_client.head_object(
    #             Bucket=self.bucket_name,
    #             Key=f"{self.user_prefix}{file_name}"
    #         )
    #         file_size = response['ContentLength']
            
    #         # Delete file
    #         self.s3_client.delete_object(
    #             Bucket=self.bucket_name,
    #             Key=f"{self.user_prefix}{file_name}"
    #         )
            
    #         # Update storage usage
    #         storage = UserStorage.objects.get(user=self.user)
    #         storage.storage_used = max(0, storage.storage_used - file_size)
    #         storage.save()
            
    #         return True
    #     except Exception as e:
    #         raise Exception(f"Error deleting file: {str(e)}")
        
    # def delete_file(self, file_name):
    #     """Delete file from user's S3 directory"""
    #     try:
    #         s3_key = f"{self.user_prefix}{file_name}"
            
    #         # Verify file belongs to user
    #         try:
    #             response = self.s3_client.head_object(
    #                 Bucket=self.bucket_name,
    #                 Key=s3_key
    #             )
    #             if response['Metadata'].get('user_id') != str(self.user.id):
    #                 raise Exception("Unauthorized access to file")
                
    #             file_size = response['ContentLength']
    #         except self.s3_client.exceptions.ClientError:
    #             raise Exception("File not found")

    #         # Delete file
    #         self.s3_client.delete_object(
    #             Bucket=self.bucket_name,
    #             Key=s3_key
    #         )
            
    #         # Update storage usage
    #         storage = UserStorage.objects.get(user=self.user)
    #         storage.storage_used = max(0, storage.storage_used - file_size)
    #         storage.save()
            
    #         return True
    #     except Exception as e:
    #         raise Exception(f"Error deleting file: {str(e)}")
        
    def generate_download_url(self, s3_key, expires_in=3600):
        """
        Generate a download URL for a file in S3
        
        Args:
            s3_key (str): The S3 key of the file
            expires_in (int): Number of seconds until the URL expires
            
        Returns:
            str: The download URL
        """
        try:
            params = {
                'Bucket': self.bucket_name,
                'Key': s3_key,
                'ResponseContentDisposition': f'attachment; filename="{s3_key.split("/")[-1]}"'
            }
            
            url = self.s3_client.generate_presigned_url(
                ClientMethod='get_object',
                Params=params,
                ExpiresIn=expires_in
            )
            
            return url
        except Exception as e:
            logger.exception(f"Error generating download URL for {s3_key}: {str(e)}")
            raise Exception(f"Could not generate URL: {str(e)}")

    def file_exists(self, s3_key):
        """Check if a file exists in S3"""
        try:
            self.s3_client.head_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )
            return True
        except self.s3_client.exceptions.NoSuchKey:
            return False
        except Exception as e:
            print(f"Error checking if file exists in S3: {str(e)}")
            return False
    
    def delete_file(self, s3_key):
        """Delete file from S3 with better error handling"""
        try:
            # Check if file exists first
            if not self.file_exists(s3_key):
                raise FileNotFoundError(f"File not found in S3: {s3_key}")
            
            # Delete the file
            self.s3_client.delete_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )
            print(f"Successfully deleted file from S3: {s3_key}")
            return True
            
        except FileNotFoundError:
            raise  # Re-raise file not found errors
        except Exception as e:
            print(f"Error deleting file from S3: {str(e)}")
            raise Exception(f"Error deleting file: {str(e)}")
    
    def list_user_files_with_details(self):
        """List all files for user with details for debugging"""
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=self.user_prefix
            )
            
            files = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    files.append({
                        'key': obj['Key'],
                        'size': obj['Size'],
                        'last_modified': obj['LastModified'],
                    })
            
            return files
        except Exception as e:
            print(f"Error listing user files: {str(e)}")
            return []

================
File: views.py
================
from django.http import JsonResponse
from .utils import S3StorageManager
from django.contrib.auth.decorators import login_required
from rest_framework import viewsets, status
from rest_framework.decorators import api_view, permission_classes, action
from rest_framework.permissions import IsAuthenticated, IsAdminUser
from rest_framework.response import Response
from django.db.models import Count, Sum
from django.utils import timezone
from datetime import timedelta
from .models import UserStorage, AdminAccessLog
from .serializers import (
    StorageInfoSerializer, StorageAnalyticsSerializer,
    AdminAccessLogSerializer, StorageOptimizationSerializer
)

from django.db.models import Sum, Count, Avg
from django.db.models.functions import TruncMonth
from file_management.models import UserFile, FileCategory
# @login_required
# def get_storage_info(request):
#     try:
#         storage_manager = S3StorageManager(request.user)
#         storage_info = storage_manager.get_user_storage_info()
        
#         # Convert bytes to more readable format
#         def format_size(size):
#             for unit in ['B', 'KB', 'MB', 'GB']:
#                 if size < 1024:
#                     return f"{size:.2f} {unit}"
#                 size /= 1024
#             return f"{size:.2f} TB"
        
#         return JsonResponse({
#             'used': format_size(storage_info['used']),
#             'limit': format_size(storage_info['limit']),
#             'available': format_size(storage_info['available']),
#             'percentage_used': f"{storage_info['percentage_used']:.2f}%",
#             'raw': {
#                 'used': storage_info['used'],
#                 'limit': storage_info['limit'],
#                 'available': storage_info['available']
#             }
#         })
#     except Exception as e:
#         return JsonResponse({
#             'error': str(e)
#         }, status=500)
    


class StorageViewSet(viewsets.ModelViewSet):
    permission_classes = [IsAuthenticated]
    serializer_class = StorageInfoSerializer

    def get_queryset(self):
        return UserStorage.objects.filter(user=self.request.user)

    def get_object(self):
        storage = self.get_queryset().first()
        if not storage:
            # Create missing storage record
            storage, created = UserStorage.objects.get_or_create(
                user=self.request.user,
                defaults={
                    'storage_used': 0,
                    'storage_limit': 5368709120  # 5GB default
                }
            )
            if created:
                print(f"[StorageViewSet] Created missing UserStorage for user {self.request.user.email}")
                # Update storage limit based on subscription if available
                try:
                    storage.update_from_subscription()
                except Exception as e:
                    print(f"[StorageViewSet] Could not update subscription for {self.request.user.email}: {e}")
        return storage

    def list(self, request):
        storage = self.get_object()
        if not storage:
            return Response({'error': 'Storage not found'}, 
                          status=status.HTTP_404_NOT_FOUND)
        
        # Update storage limit from subscription if needed
        storage.update_from_subscription()
        
        serializer = self.get_serializer(storage)
        data = serializer.data
        
        # Add subscription info for frontend
        try:
            from payments.utils import get_user_subscription_info
            subscription_info = get_user_subscription_info(request.user)
            data['subscription_info'] = subscription_info
        except ImportError:
            data['subscription_info'] = {'is_sparkle': False}
        
        return Response(data)

    @action(detail=False, methods=['get'])
    def analytics(self, request):
        """
        Enhanced analytics with sparkle-specific features
        """
        user = self.request.user
        
        # Get subscription info for sparkle features
        try:
            from payments.utils import get_user_subscription_info
            subscription_info = get_user_subscription_info(user)
            is_sparkle = subscription_info.get('is_sparkle', False)
        except ImportError:
            is_sparkle = False
        
        # 1. Storage Overview
        try:
            storage = UserStorage.objects.get(user=user)
        except UserStorage.DoesNotExist:
            # Create missing storage record
            storage, created = UserStorage.objects.get_or_create(
                user=user,
                defaults={
                    'storage_used': 0,
                    'storage_limit': 5368709120  # 5GB default
                }
            )
            if created:
                print(f"[StorageViewSet.analytics] Created missing UserStorage for user {user.email}")
        
        storage.update_from_subscription()  # Ensure up-to-date limits
        
        storage_overview = {
            'used': storage.storage_used,
            'limit': storage.storage_limit,
            'percentage_used': storage.get_usage_percentage(),
            'percentage_left': 100 - storage.get_usage_percentage(),
            'is_sparkle': is_sparkle
        }

        # 2. Storage Breakdown by Category
        category_usage = UserFile.objects.filter(user=user)\
            .values('category__name')\
            .annotate(total_size=Sum('file_size'))\
            .order_by('-total_size')

        storage_breakdown = [
            {'category': item['category__name'] or 'Uncategorized', 'size': item['total_size']}
            for item in category_usage
        ]

        # 3. Enhanced Monthly Trends (more detailed for sparkle users)
        months_back = 12 if is_sparkle else 6
        start_date = timezone.now() - timedelta(days=months_back*30)
        
        monthly_data = UserFile.objects.filter(user=user, upload_date__gte=start_date)\
            .annotate(month=TruncMonth('upload_date'))\
            .values('month', 'category__name')\
            .annotate(count=Count('id'), total_size=Sum('file_size'))\
            .order_by('month')

        # Process trends data
        trends = {}
        for item in monthly_data:
            category = item['category__name'] or 'Uncategorized'
            if category not in trends:
                trends[category] = []
            
            trends[category].append({
                'month': item['month'].strftime('%Y-%m-%d'),
                'month_short': item['month'].strftime('%b'),
                'count': item['count'],
                'size': item['total_size'] if is_sparkle else None  # Size data only for sparkle
            })

        # 4. Sparkle-exclusive analytics
        sparkle_analytics = {}
        if is_sparkle:
            # File type distribution
            file_types = UserFile.objects.filter(user=user)\
                .values('file_type')\
                .annotate(count=Count('id'), total_size=Sum('file_size'))\
                .order_by('-total_size')[:10]
            
            sparkle_analytics = {
                'file_types': list(file_types),
                'total_files': UserFile.objects.filter(user=user).count(),
                'average_file_size': UserFile.objects.filter(user=user).aggregate(
                    avg_size= Avg('file_size')
                )['avg_size'] or 0,
            }

        response_data = {
            'storage_overview': storage_overview,
            'storage_breakdown': storage_breakdown,
            'monthly_trends': trends,
            'is_sparkle': is_sparkle,
            'sparkle_analytics': sparkle_analytics if is_sparkle else None
        }

        return Response(response_data)

    @action(detail=False, methods=['get'])
    def optimization(self, request):
        """
        Enhanced storage optimization with sparkle-specific features
        """
        try:
            from payments.utils import get_user_subscription_info
            subscription_info = get_user_subscription_info(request.user)
            is_sparkle = subscription_info.get('is_sparkle', False)
        except ImportError:
            is_sparkle = False

        files = UserFile.objects.filter(user=request.user)
        
        # Basic optimization (all users)
        large_files = files.filter(
            file_size__gt=100*1024*1024  # >100MB
        ).values('id', 'original_filename', 'file_size')
        
        # Enhanced optimization for sparkle users
        if is_sparkle:
            # More detailed duplicate detection
            duplicates = files.values(
                'file_size', 'file_type', 'file_hash'  # Assuming you have file_hash
            ).annotate(
                count=Count('id')
            ).filter(count__gt=1)
            
            # Detailed old files analysis
            six_months_ago = timezone.now() - timedelta(days=180)
            one_year_ago = timezone.now() - timedelta(days=365)
            
            old_files = {
                'six_months': files.filter(upload_date__lt=six_months_ago).count(),
                'one_year': files.filter(upload_date__lt=one_year_ago).count(),
                'details': files.filter(upload_date__lt=six_months_ago).values(
                    'id', 'original_filename', 'upload_date', 'file_size'
                )[:50]  # Limit for performance
            }
            
            recommendations = [
                'Consider compressing large files to save space',
                'Remove duplicate files to optimize storage',
                'Archive old files to external storage',
                'Use appropriate file formats for your content',
                'Regularly review and clean up unused files',
                'Consider upgrading storage if consistently near limit'
            ]
        else:
            # Basic duplicate detection
            duplicates = files.values(
                'file_size', 'file_type'
            ).annotate(
                count=Count('id')
            ).filter(count__gt=1)
            
            six_months_ago = timezone.now() - timedelta(days=180)
            old_files = files.filter(upload_date__lt=six_months_ago).values(
                'id', 'original_filename', 'upload_date'
            )[:10]  # Limited for basic users
            
            recommendations = [
                'Consider upgrading to Sparkle plan for advanced optimization',
                'Remove large files you no longer need',
                'Delete duplicate files'
            ]
        
        # Calculate potential savings
        potential_savings = sum(f['file_size'] for f in large_files)
        if duplicates:
            duplicate_savings = sum(
                d['file_size'] * (d['count'] - 1) 
                for d in duplicates if d['file_size']
            )
            potential_savings += duplicate_savings
        
        data = {
            'large_files': list(large_files),
            'duplicate_files': list(duplicates),
            'old_files': old_files,
            'potential_savings': potential_savings,
            'recommendations': recommendations,
            'is_sparkle': is_sparkle
        }
        
        serializer = StorageOptimizationSerializer(data)
        return Response(serializer.data)

class AdminAccessLogViewSet(viewsets.ReadOnlyModelViewSet):
    permission_classes = [IsAdminUser]
    serializer_class = AdminAccessLogSerializer
    queryset = AdminAccessLog.objects.all()

    def get_queryset(self):
        queryset = AdminAccessLog.objects.all()
        
        # Filter by date range
        start_date = self.request.query_params.get('start_date')
        end_date = self.request.query_params.get('end_date')
        if start_date and end_date:
            queryset = queryset.filter(
                access_time__range=[start_date, end_date]
            )
        
        # Filter by admin user
        admin_user = self.request.query_params.get('admin_user')
        if admin_user:
            queryset = queryset.filter(admin_user__username=admin_user)
        
        # Filter by access type
        access_type = self.request.query_params.get('access_type')
        if access_type:
            queryset = queryset.filter(access_type=access_type)
        
        return queryset.order_by('-access_time')
    

from django.views.decorators.csrf import csrf_exempt

@csrf_exempt
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def get_storage_info(request):
    try:
        storage_manager = S3StorageManager(request.user)
        storage_info = storage_manager.get_user_storage_info()
        
        # Convert bytes to more readable format
        def format_size(size):
            for unit in ['B', 'KB', 'MB', 'GB']:
                if size < 1024:
                    return f"{size:.2f} {unit}"
                size /= 1024
            return f"{size:.2f} TB"
        
        return Response({
            'used': format_size(storage_info['used']),
            'limit': format_size(storage_info['limit']),
            'available': format_size(storage_info['available']),
            'percentage_used': f"{storage_info['percentage_used']:.2f}%",
            'raw': {
                'used': storage_info['used'],
                'limit': storage_info['limit'],
                'available': storage_info['available'],
                'percentage_used': storage_info['percentage_used']
            }
        })
    except Exception as e:
        return Response({
            'error': str(e)
        }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
